{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be41e02e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\10\\PROGRAMMS\\anaconda\\envs\\main\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from scipy.linalg import norm\n",
    "from scipy import stats\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity,cosine_distances\n",
    "import string\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e836bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем стоп слова для русского и английского языка\n",
    "stop1 = list(stopwords.words('english'))\n",
    "stop2 = list(stopwords.words('russian'))\n",
    "\n",
    "stop = stop1 + stop2\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "# Функция для предобработки. Удаление знаков пуктуации, приведение к нижнему регистру, лемматизация.\n",
    "def preprocessing(line):\n",
    "    line = line.lower()\n",
    "    line = re.sub(r'[.,\"\\'-?:!;]', \"\", line)\n",
    "    line = nltk.word_tokenize(line)\n",
    "    line = ' '.join([lemmatizer.lemmatize(w) for w in line])\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56ea7a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Читаем наш датасет.\n",
    "df = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "418d5e37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pair_id</th>\n",
       "      <th>name_1</th>\n",
       "      <th>name_2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Iko Industries Ltd.</td>\n",
       "      <td>Enormous Industrial Trade Pvt., Ltd.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Apcotex Industries Ltd.</td>\n",
       "      <td>Technocraft Industries (India) Ltd.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Rishichem Distributors Pvt., Ltd.</td>\n",
       "      <td>Dsa</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Powermax Rubber Factory</td>\n",
       "      <td>Co. One</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Tress A/S</td>\n",
       "      <td>Longyou Industries Park Zhejiang</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pair_id                             name_1  \\\n",
       "0        1                Iko Industries Ltd.   \n",
       "1        2            Apcotex Industries Ltd.   \n",
       "2        3  Rishichem Distributors Pvt., Ltd.   \n",
       "3        4            Powermax Rubber Factory   \n",
       "4        5                          Tress A/S   \n",
       "\n",
       "                                 name_2  is_duplicate  \n",
       "0  Enormous Industrial Trade Pvt., Ltd.             0  \n",
       "1   Technocraft Industries (India) Ltd.             0  \n",
       "2                                   Dsa             0  \n",
       "3                               Co. One             0  \n",
       "4      Longyou Industries Park Zhejiang             0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Посмотрим на данные.\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db363206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 497819 entries, 0 to 497818\n",
      "Data columns (total 4 columns):\n",
      " #   Column        Non-Null Count   Dtype \n",
      "---  ------        --------------   ----- \n",
      " 0   pair_id       497819 non-null  int64 \n",
      " 1   name_1        497819 non-null  object\n",
      " 2   name_2        497819 non-null  object\n",
      " 3   is_duplicate  497819 non-null  int64 \n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 15.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "373bd832",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    494161\n",
       "1      3658\n",
       "Name: is_duplicate, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Посмотрим сколько одинаковых пар, а сколько разных.\n",
    "df.is_duplicate.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce065a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разбиваем датафрейм на на части с копиями и без них.\n",
    "\n",
    "df_0 = df[df['is_duplicate'] == 0]\n",
    "df_1 = df[df['is_duplicate'] == 1]\n",
    "\n",
    "name_1_df_0 = list(df_0['name_1'])\n",
    "name_2_df_0 = list(df_0['name_2'])\n",
    "\n",
    "name_1_df_1 = list(df_1['name_1'])\n",
    "name_2_df_1 = list(df_1['name_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77c53cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "995638\n"
     ]
    }
   ],
   "source": [
    "# Объединим два столбца, посмотрим сколько всего названий.\n",
    "text = list(df['name_1']) + list(list(df['name_2']))\n",
    "\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2922323c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество уникальных названий   18022\n",
      "Количество слов 73720\n",
      "Количество уникальных слов  16046\n"
     ]
    }
   ],
   "source": [
    "# Оставим только уникальные названия.\n",
    "\n",
    "text_unic = list(set(text))\n",
    "\n",
    "print(\"Количество уникальных названий  \" , len(text_unic))\n",
    "\n",
    "# Разобъем все названия на отдельные слова\n",
    "words = []\n",
    "for string in text_unic:\n",
    "    subwords = preprocessing(string).split()\n",
    "    for word in subwords:\n",
    "        words.append(word)\n",
    "\n",
    "print(\"Количество слов\" , len(words))\n",
    "words_unic = list(set(words))\n",
    "print(\"Количество уникальных слов \", len(words_unic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86c951e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bestocean worldwide logistics inc\n"
     ]
    }
   ],
   "source": [
    "print(preprocessing(text[12]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d9f8e220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Оставим только слова длиннее 1 символа.\n",
    "filtered_words = [w for w in words_unic if len(w) > 1]\n",
    "\n",
    "# Загрузим предобученную модель word2vec на полтора гигабайта\n",
    "\n",
    "glove = api.load(\"glove-twitter-200\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f63929dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция делает эмбеддинг из строки, как сумму эмбеддингов входящих в строку слов\n",
    "def sentence_embedding(string):\n",
    "    sentence_list = [glove.get_vector(w) if w in glove else np.zeros(200) for w in string.split()  ]\n",
    "    result = np.sum(np.array(sentence_list), axis=0)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f3a6781c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "carlisle coating & waterproofing inc\n",
      "carlisle coating & wtrprfng\n",
      "\n",
      "iko industry ltd\n",
      "enormous industrial trade pvt ltd\n"
     ]
    }
   ],
   "source": [
    "# Пример текста. Предобработаем его, вычислим эмбеддинги, а потом посмотрим на косинусное сходство.\n",
    "text1 = preprocessing('Carlisle Coatings & Waterproofing, Inc.' )\n",
    "text2 = preprocessing('Carlisle Coatings & Wtrprfng')\n",
    "\n",
    "print(text1)\n",
    "print(text2)\n",
    "print()\n",
    "\n",
    "text1_vec = sentence_embedding(text1)\n",
    "text2_vec = sentence_embedding(text2)\n",
    "\n",
    "\n",
    "text2_1 = preprocessing('Iko Industries Ltd.' )\n",
    "text2_2 = preprocessing('Enormous Industrial Trade Pvt., Ltd.')\n",
    "\n",
    "print(text2_1)\n",
    "print(text2_2)\n",
    "\n",
    "text2_1_vec = sentence_embedding(text2_1)\n",
    "text2_2_vec = sentence_embedding(text2_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a9e81574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.84703398]]\n",
      "\n",
      "[[0.67530155]]\n"
     ]
    }
   ],
   "source": [
    "# Посмотрим на косинусное сходство двух текстов.\n",
    "print(cosine_similarity(text1_vec.reshape(1, -1), text2_vec.reshape(1, -1)))\n",
    "print()\n",
    "print(cosine_similarity(text2_1_vec.reshape(1, -1), text2_2_vec.reshape(1, -1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2338a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вычислим косинусное сходство для всех пар из нашего датасета.\n",
    "\n",
    "pair_0 = [cosine_similarity(sentence_embedding(item[0]).reshape(1, -1), sentence_embedding(item[1]).reshape(1, -1)) for item in zip(name_1_df_0 ,name_2_df_0 )]\n",
    "\n",
    "pair_1 = [cosine_similarity(sentence_embedding(item[0]).reshape(1, -1), sentence_embedding(item[1]).reshape(1, -1)) for item in zip(name_1_df_1 ,name_2_df_1 )]\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "875d0f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.89154127]]), array([[0.]]), array([[0.]]), array([[0.]])]\n"
     ]
    }
   ],
   "source": [
    "# Посмотрим на косинусное сходство первых 50 непохожиш пар.\n",
    "print(pair_0[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c5399d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[1.]]), array([[0.]]), array([[0.]]), array([[0.33409574]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[1.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[1.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.77249323]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]])]\n"
     ]
    }
   ],
   "source": [
    "# Посмотрим на косинусное сходство первых 50 похожиш пар.\n",
    "print(pair_1[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "788e63cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "494161\n",
      "3658\n"
     ]
    }
   ],
   "source": [
    "print(len(pair_0))\n",
    "print(len(pair_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a209dbff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "486853    98.52112975325855\n",
      "75    2.050300710770913\n"
     ]
    }
   ],
   "source": [
    "# Выведем долю пар схожих и отличных по косинусному сходству по порогу 0.3\n",
    "\n",
    "pair_0_trashold_05 = [i for i in pair_0 if i < 0.3]\n",
    "pair_1_trashold_05 = [i for i in pair_1 if i>= 0.3]\n",
    "\n",
    "print(len(pair_0_trashold_05), \"  \",len(pair_0_trashold_05)/len(pair_0)*100)\n",
    "\n",
    "print(len(pair_1_trashold_05), \"  \",len(pair_1_trashold_05)/len(pair_1)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088458ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
