{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a433dd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from scipy.linalg import norm\n",
    "from scipy import stats\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity,cosine_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23cdb5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем стоп слова для русского и английского языка\n",
    "stop1 = list(stopwords.words('english'))\n",
    "stop2 = list(stopwords.words('russian'))\n",
    "stop = stop1 + stop2\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Функция для предобработки. Удаление знаков пуктуации, приведение к нижнему регистру, лемматизация.\n",
    "def preprocessing(line):\n",
    "    line = line.lower()\n",
    "    line = re.sub(r'[.,\"\\'-?:!;]', \"\", line)\n",
    "    line = nltk.word_tokenize(line)\n",
    "    line = ' '.join([lemmatizer.lemmatize(w) for w in line])\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f156c433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Читаем наш датасет.\n",
    "df = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94d7ab5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pair_id</th>\n",
       "      <th>name_1</th>\n",
       "      <th>name_2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Iko Industries Ltd.</td>\n",
       "      <td>Enormous Industrial Trade Pvt., Ltd.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Apcotex Industries Ltd.</td>\n",
       "      <td>Technocraft Industries (India) Ltd.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Rishichem Distributors Pvt., Ltd.</td>\n",
       "      <td>Dsa</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Powermax Rubber Factory</td>\n",
       "      <td>Co. One</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Tress A/S</td>\n",
       "      <td>Longyou Industries Park Zhejiang</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pair_id                             name_1  \\\n",
       "0        1                Iko Industries Ltd.   \n",
       "1        2            Apcotex Industries Ltd.   \n",
       "2        3  Rishichem Distributors Pvt., Ltd.   \n",
       "3        4            Powermax Rubber Factory   \n",
       "4        5                          Tress A/S   \n",
       "\n",
       "                                 name_2  is_duplicate  \n",
       "0  Enormous Industrial Trade Pvt., Ltd.             0  \n",
       "1   Technocraft Industries (India) Ltd.             0  \n",
       "2                                   Dsa             0  \n",
       "3                               Co. One             0  \n",
       "4      Longyou Industries Park Zhejiang             0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Посмотрим на данные.\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e4378a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 497819 entries, 0 to 497818\n",
      "Data columns (total 4 columns):\n",
      " #   Column        Non-Null Count   Dtype \n",
      "---  ------        --------------   ----- \n",
      " 0   pair_id       497819 non-null  int64 \n",
      " 1   name_1        497819 non-null  object\n",
      " 2   name_2        497819 non-null  object\n",
      " 3   is_duplicate  497819 non-null  int64 \n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 15.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b41ad3c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    494161\n",
       "1      3658\n",
       "Name: is_duplicate, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Посмотрим сколько одинаковых пар, а сколько разных.\n",
    "df.is_duplicate.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01bb7e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разбиваем датафрейм на на части с копиями и без них.\n",
    "\n",
    "df_0 = df[df['is_duplicate'] == 0]\n",
    "df_1 = df[df['is_duplicate'] == 1]\n",
    "\n",
    "name_1_df_0 = list(df_0['name_1'])\n",
    "name_2_df_0 = list(df_0['name_2'])\n",
    "\n",
    "name_1_df_1 = list(df_1['name_1'])\n",
    "name_2_df_1 = list(df_1['name_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66d62010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "995638\n"
     ]
    }
   ],
   "source": [
    "# Объединим два столбца, посмотрим сколько всего названий.\n",
    "text = list(df['name_1']) + list(list(df['name_2']))\n",
    "\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44dd5609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18022\n"
     ]
    }
   ],
   "source": [
    "# Оставим только уникальные названия.\n",
    "text_unic = list(set(text))\n",
    "\n",
    "print(len(text_unic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3a10a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bestocean worldwide logistics inc\n"
     ]
    }
   ],
   "source": [
    "# Посмотрим на результат предобработки\n",
    "print(preprocessing(text[12]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bbde6d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\10\\PROGRAMMS\\anaconda\\envs\\main\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['arent', 'couldnt', 'didnt', 'doe', 'doesnt', 'dont', 'ha', 'hadnt', 'hasnt', 'havent', 'isnt', 'mightnt', 'mustnt', 'neednt', 'shant', 'shes', 'shouldnt', 'shouldve', 'thatll', 'wa', 'wasnt', 'werent', 'wont', 'wouldnt', 'youd', 'youll', 'youre', 'youve'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(preprocessor=<function preprocessing at 0x000001E4556899D0>,\n",
       "                stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                            'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                            \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                            'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                            'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                            'itself', ...])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Будем делать эмбеддинги с помощью TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(preprocessor=preprocessing, analyzer = 'word',stop_words=stop)\n",
    "\n",
    "tfidf_vectorizer.fit(text_unic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7b0a8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "carlisle coating & waterproofing inc\n",
      "carlisle coating & wtrprfng\n"
     ]
    }
   ],
   "source": [
    "# Пример текста. Предобработаем его, вычислим эмбеддинги, а потом посмотрим на косинусное сходство.\n",
    "text1 = preprocessing('Carlisle Coatings & Waterproofing, Inc.' )\n",
    "text2 = preprocessing('Carlisle Coatings & Wtrprfng')\n",
    "\n",
    "print(text1)\n",
    "print(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c826dac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 15932)\n",
      "(1, 15932)\n"
     ]
    }
   ],
   "source": [
    "# Сделаем эмбеддинги двух примеров названий.\n",
    "text1_vec = tfidf_vectorizer.transform([text1])\n",
    "text2_vec = tfidf_vectorizer.transform([text2])\n",
    "\n",
    "print(text1_vec.shape)\n",
    "print(text2_vec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b0b2a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.58029538]]\n",
      "0.5802953765169023\n"
     ]
    }
   ],
   "source": [
    "# Посмотрим на косинусное сходство двух схожих текстов.\n",
    "print(cosine_similarity(text1_vec, text2_vec))\n",
    "print(cosine_similarity(text1_vec, text2_vec)[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "314b3eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.03765532]]\n"
     ]
    }
   ],
   "source": [
    "# Посмотрим на косинусное сходство двух несхожих текстов.\n",
    "print(cosine_similarity(tfidf_vectorizer.transform(['Iko Industries Ltd.']), tfidf_vectorizer.transform(['Enormous Industrial Trade Pvt., Ltd.'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdac4be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вычислим косинусное сходство для всех пар из нашего датасета.\n",
    "\n",
    "\n",
    "def prep(text):\n",
    "    return tfidf_vectorizer.transform([preprocessing(text)])\n",
    "\n",
    "\n",
    "pair_0 = []\n",
    "pair_1 = []\n",
    "\n",
    "for i in range(len(name_1_df_0) ):\n",
    "    \n",
    "    pair_0.append( cosine_similarity( prep(name_1_df_0[i] ),prep(name_2_df_0[i])  ))\n",
    "\n",
    "    \n",
    "for i in range(len(name_1_df_1) ):\n",
    "    \n",
    "    pair_1.append( cosine_similarity( prep(name_1_df_1[i] ),prep(name_2_df_1[i])  ))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9ebe6cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0.03765532]]), array([[0.19197232]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.34142763]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.12334998]]), array([[0.12495284]]), array([[0.06036677]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.10729315]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.13716193]]), array([[0.15815033]]), array([[0.13965938]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.19253311]]), array([[0.]]), array([[0.10244252]]), array([[0.29215175]]), array([[0.17287329]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.04771968]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.17103228]]), array([[0.]]), array([[0.]]), array([[0.32738178]]), array([[0.]]), array([[0.]]), array([[0.]])]\n"
     ]
    }
   ],
   "source": [
    "# Посмотрим на косинусное сходство первых 50 непохожиш пар.\n",
    "print(pair_0[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b201200e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0.77822582]]), array([[0.4210751]]), array([[0.55818533]]), array([[0.52995888]]), array([[0.1541044]]), array([[0.25176037]]), array([[0.56044089]]), array([[0.58029538]]), array([[0.35928688]]), array([[0.55887625]]), array([[0.77159317]]), array([[0.48812509]]), array([[0.33527645]]), array([[0.41091764]]), array([[0.48404833]]), array([[0.79613119]]), array([[1.]]), array([[0.24603778]]), array([[0.33458772]]), array([[0.80027732]]), array([[0.57447577]]), array([[0.54399832]]), array([[0.37463725]]), array([[0.48365744]]), array([[0.33466634]]), array([[0.64760312]]), array([[1.]]), array([[0.80039734]]), array([[1.]]), array([[0.37745632]]), array([[0.54938373]]), array([[0.34161408]]), array([[0.6814642]]), array([[0.40513767]]), array([[0.]]), array([[0.47853116]]), array([[0.48828846]]), array([[0.91170319]]), array([[0.55400584]]), array([[0.88026406]]), array([[0.87559488]]), array([[0.16927868]]), array([[0.4057115]]), array([[0.31095996]]), array([[0.39413544]]), array([[0.90546095]]), array([[0.26540674]]), array([[0.37501599]]), array([[0.7611166]]), array([[0.55205724]])]\n"
     ]
    }
   ],
   "source": [
    "# Посмотрим на косинусное сходство первых 50 похожиш пар.\n",
    "print(pair_1[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9ffb6d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "494161\n",
      "3658\n"
     ]
    }
   ],
   "source": [
    "print(len(pair_0))\n",
    "print(len(pair_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7a6e25fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "463144    93.72330070564047\n",
      "3102    84.80043739748496\n"
     ]
    }
   ],
   "source": [
    "# Выведем долю пар схожих и отличных по косинусному сходству по порогу 0.3\n",
    "\n",
    "pair_0_trashold_05 = [i for i in pair_0 if i < 0.3]\n",
    "pair_1_trashold_05 = [i for i in pair_1 if i>= 0.3]\n",
    "\n",
    "print(len(pair_0_trashold_05), \"  \",len(pair_0_trashold_05)/len(pair_0)*100)\n",
    "\n",
    "print(len(pair_1_trashold_05), \"  \",len(pair_1_trashold_05)/len(pair_1)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d30ce6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
