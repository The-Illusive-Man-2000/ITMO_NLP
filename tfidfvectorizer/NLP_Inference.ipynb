{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ddd03a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle\n",
    "from heapq import nlargest\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import session_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b4014b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<details>\n",
       "<summary>Click to view session information</summary>\n",
       "<pre>\n",
       "-----\n",
       "nltk                3.6.2\n",
       "numpy               1.21.5\n",
       "pandas              1.4.2\n",
       "session_info        1.0.0\n",
       "sklearn             1.0.2\n",
       "-----\n",
       "</pre>\n",
       "<details>\n",
       "<summary>Click to view modules imported as dependencies</summary>\n",
       "<pre>\n",
       "asttokens                   NA\n",
       "backcall                    0.2.0\n",
       "beta_ufunc                  NA\n",
       "binom_ufunc                 NA\n",
       "bottleneck                  1.3.4\n",
       "cloudpickle                 2.0.0\n",
       "colorama                    0.4.4\n",
       "cython_runtime              NA\n",
       "dateutil                    2.8.2\n",
       "debugpy                     1.5.1\n",
       "decorator                   5.1.1\n",
       "entrypoints                 0.4\n",
       "executing                   0.8.3\n",
       "google                      NA\n",
       "importlib_metadata          NA\n",
       "ipykernel                   6.9.1\n",
       "ipython_genutils            0.2.0\n",
       "jedi                        0.18.1\n",
       "joblib                      1.1.0\n",
       "mkl                         2.4.0\n",
       "mpl_toolkits                NA\n",
       "nbinom_ufunc                NA\n",
       "nt                          NA\n",
       "ntsecuritycon               NA\n",
       "numexpr                     2.8.1\n",
       "packaging                   20.9\n",
       "parso                       0.8.3\n",
       "pickleshare                 0.7.5\n",
       "pkg_resources               NA\n",
       "prompt_toolkit              3.0.20\n",
       "psutil                      5.8.0\n",
       "pure_eval                   0.2.2\n",
       "pyarrow                     3.0.0\n",
       "pydev_ipython               NA\n",
       "pydevconsole                NA\n",
       "pydevd                      2.6.0\n",
       "pydevd_concurrency_analyser NA\n",
       "pydevd_file_utils           NA\n",
       "pydevd_plugins              NA\n",
       "pydevd_tracing              NA\n",
       "pygments                    2.11.2\n",
       "pythoncom                   NA\n",
       "pytz                        2021.3\n",
       "pywintypes                  NA\n",
       "regex                       2.5.112\n",
       "scipy                       1.7.3\n",
       "setuptools                  61.2.0\n",
       "six                         1.16.0\n",
       "stack_data                  0.2.0\n",
       "threadpoolctl               2.2.0\n",
       "tornado                     6.1\n",
       "traitlets                   5.1.1\n",
       "typing_extensions           NA\n",
       "wcwidth                     0.2.5\n",
       "win32api                    NA\n",
       "win32com                    NA\n",
       "win32security               NA\n",
       "zipp                        NA\n",
       "zmq                         22.3.0\n",
       "</pre>\n",
       "</details> <!-- seems like this ends pre, so might as well be explicit -->\n",
       "<pre>\n",
       "-----\n",
       "IPython             8.2.0\n",
       "jupyter_client      7.2.2\n",
       "jupyter_core        4.9.2\n",
       "jupyterlab          2.2.6\n",
       "notebook            6.1.6\n",
       "-----\n",
       "Python 3.8.5 (default, Sep  3 2020, 21:29:08) [MSC v.1916 64 bit (AMD64)]\n",
       "Windows-10-10.0.10586-SP0\n",
       "-----\n",
       "Session information updated at 2022-10-25 23:53\n",
       "</pre>\n",
       "</details>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Посмотрим версии используемых модулей.\n",
    "session_info.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b360e4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузим ранее сохранённые уникальные навазния компаний.\n",
    "text_unic = pickle.load(open('companies_name_unic.pickle','rb'))\n",
    "\n",
    "# Загрузим ранее сохранённый tfidfvectorizer.\n",
    "my_vectorizer = pickle.load(open('vectorizer_2.pickle','rb'))\n",
    "\n",
    "def sentence_embedding_fl(string, vectorizer):\n",
    "    tf_vectorizer = vectorizer\n",
    "    sentence_list = [tf_vectorizer.transform([word]).toarray() for word in string.split()]\n",
    "        \n",
    "    result = np.sum(np.array(sentence_list), axis=0)\n",
    "    if len(result.shape) == 0:\n",
    "        result = np.zeros((1,9776))\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07fd3751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузим ранее сохранённые эмбеддинги для названий компаний.\n",
    "name_embeddings = pickle.load(open('name_embeddings.pickle','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acc81fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Тестовый текст 1\n",
    "my_test_text_1 = 'Darth Vader is a great Sith Lord'\n",
    "\n",
    "# Получим эмбеддинг тестового текста\n",
    "text_vector = sentence_embedding_fl(my_test_text_1, my_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bb15861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вычислим косинусное сходство между тестовым текстом и навзаниями компаний.\n",
    "cosine_similarity_list = [cosine_similarity( text_vector, sentence_vector)[0] for sentence_vector in name_embeddings ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6562f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lord Industrial Ltda\n",
      "Lord India Pvt., Ltd.\n",
      "Jm Lord International Llc\n",
      "Greatech Philippines Inc.\n",
      "Great Sports Infra Pvt., Ltd.\n"
     ]
    }
   ],
   "source": [
    "top = 5\n",
    "\n",
    "# Функция для поиска топ 5 схожих названий.\n",
    "def find_top5(similarity_list, top):\n",
    "    company_names = []\n",
    "    maximums = nlargest(top, similarity_list)\n",
    "    indexes_set = set()\n",
    "    for maximum in maximums:\n",
    "        indexes = list(np.where(np.array(similarity_list) == maximum)[0])\n",
    "        for index in indexes:\n",
    "            if index not in indexes_set:\n",
    "                indexes_set.add(index)\n",
    "                company_names.append(text_unic[index])\n",
    "                \n",
    "    return  company_names                   \n",
    "\n",
    "\n",
    "names = find_top5(cosine_similarity_list, top)\n",
    "N = len(names)\n",
    "\n",
    "# Так как одно косинусное сходство может быть у нескольких элементов, то список может быть больше 5.\n",
    "# Поэтому будем выводить все похожие названия, если найдено меньше 5 и только 5, если больше.\n",
    "if N<=5:\n",
    "    for name in names:\n",
    "        print(name)\n",
    "else:\n",
    "    for name in names[:5]:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f466cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Видим, что к непохожему на название компании тексту нашлись названия, где фигурируют похожие слова \"Lord\" и \"Great\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9acaf23f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Powermax Rubber Factory\n",
      "B G N Rubber Factory\n",
      "Western Rubbers India Pvt., Ltd.\n",
      "K.S. Rubbers\n",
      "Imp. Rubbers International Llc\n"
     ]
    }
   ],
   "source": [
    "# Теперь попробуем текст, который был в датасете.\n",
    "my_test_text_2 = 'Powermax Rubber Factory'\n",
    "\n",
    "# Получим эмбеддинг второго тестового текста\n",
    "text_vector2 = sentence_embedding_fl(my_test_text_2, my_vectorizer)\n",
    "\n",
    "# Вычислим косинусное сходство между тестовым текстом и навзаниями компаний.\n",
    "cosine_similarity_list2 = [cosine_similarity( text_vector2, sentence_vector)[0] for sentence_vector in name_embeddings ]\n",
    "\n",
    "names2 = find_top5(cosine_similarity_list2, top)\n",
    "N2 = len(names2)\n",
    "\n",
    "# Так как одно косинусное сходство может быть у нескольких элементов, то список может быть больше 5.\n",
    "# Поэтому будем выводить все похожие названия, если найдено меньше 5 и только 5, если больше.\n",
    "if N2<=5:\n",
    "    for name in names2:\n",
    "        print(name)\n",
    "else:\n",
    "    for name in names2[:5]:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005ca458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Видим что на первом месте исходный текст (не удивительно), а дальше тексты с похожими словами. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd36ee1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Honeywell Aerospace Systems Laboratory Co. S. De .R.L. De C.V.\n",
      "Aerocosta Global Systems Inc.\n",
      "Craft Colombia Sas\n",
      "Mx Systems International P Ltd.\n",
      "Craft Argentina S.A.\n"
     ]
    }
   ],
   "source": [
    "# Теперь попробуем текст, не был в датасете, но похож, на то, что был там.\n",
    "# Оригинал: Honeywell Aerospace Systems Laboratory Co. S. De .R.L. De C.V.\n",
    "# Изменённый вариант\n",
    "my_test_text_3 = 'Well Aero System Lab Industrial Craft'\n",
    "\n",
    "# Получим эмбеддинг второго тестового текста\n",
    "text_vector3 = sentence_embedding_fl(my_test_text_3, my_vectorizer)\n",
    "\n",
    "# Вычислим косинусное сходство между тестовым текстом и навзаниями компаний.\n",
    "cosine_similarity_list3 = [cosine_similarity( text_vector3, sentence_vector)[0] for sentence_vector in name_embeddings ]\n",
    "\n",
    "names3 = find_top5(cosine_similarity_list3, top)\n",
    "N3 = len(names3)\n",
    "\n",
    "# Так как одно косинусное сходство может быть у нескольких элементов, то список может быть больше 5.\n",
    "# Поэтому будем выводить все похожие названия, если найдено меньше 5 и только 5, если больше.\n",
    "if N3<=5:\n",
    "    for name in names3:\n",
    "        print(name)\n",
    "else:\n",
    "    for name in names3[:5]:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf88577b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Видим что на первом месте текст, который был изменён а дальше тексты с похожими словами. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
