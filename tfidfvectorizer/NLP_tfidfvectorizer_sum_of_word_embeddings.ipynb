{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b676467c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from scipy.linalg import norm\n",
    "from scipy import stats\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity,cosine_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c268e7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем стоп слова для русского и английского языка\n",
    "stop1 = list(stopwords.words('english'))\n",
    "stop2 = list(stopwords.words('russian'))\n",
    "\n",
    "stop = stop1 + stop2\n",
    "\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Функция для предобработки. Удаление знаков пуктуации, приведение к нижнему регистру, лемматизация.\n",
    "def preprocessing(line):\n",
    "    line = line.lower()\n",
    "    line = re.sub(r'[.,\"\\'-?:!;]', \"\", line)\n",
    "    line = nltk.word_tokenize(line)\n",
    "    line = ' '.join([lemmatizer.lemmatize(w) for w in line])\n",
    "    return line\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee1db5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Читаем наш датасет.\n",
    "df = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ecea2cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pair_id</th>\n",
       "      <th>name_1</th>\n",
       "      <th>name_2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Iko Industries Ltd.</td>\n",
       "      <td>Enormous Industrial Trade Pvt., Ltd.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Apcotex Industries Ltd.</td>\n",
       "      <td>Technocraft Industries (India) Ltd.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Rishichem Distributors Pvt., Ltd.</td>\n",
       "      <td>Dsa</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Powermax Rubber Factory</td>\n",
       "      <td>Co. One</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Tress A/S</td>\n",
       "      <td>Longyou Industries Park Zhejiang</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pair_id                             name_1  \\\n",
       "0        1                Iko Industries Ltd.   \n",
       "1        2            Apcotex Industries Ltd.   \n",
       "2        3  Rishichem Distributors Pvt., Ltd.   \n",
       "3        4            Powermax Rubber Factory   \n",
       "4        5                          Tress A/S   \n",
       "\n",
       "                                 name_2  is_duplicate  \n",
       "0  Enormous Industrial Trade Pvt., Ltd.             0  \n",
       "1   Technocraft Industries (India) Ltd.             0  \n",
       "2                                   Dsa             0  \n",
       "3                               Co. One             0  \n",
       "4      Longyou Industries Park Zhejiang             0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Посмотрим на данные.\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dab05fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 497819 entries, 0 to 497818\n",
      "Data columns (total 4 columns):\n",
      " #   Column        Non-Null Count   Dtype \n",
      "---  ------        --------------   ----- \n",
      " 0   pair_id       497819 non-null  int64 \n",
      " 1   name_1        497819 non-null  object\n",
      " 2   name_2        497819 non-null  object\n",
      " 3   is_duplicate  497819 non-null  int64 \n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 15.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4d70ca1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    494161\n",
       "1      3658\n",
       "Name: is_duplicate, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Посмотрим сколько одинаковых пар, а сколько разных.\n",
    "df.is_duplicate.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2cd71ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разбиваем датафрейм на на части с копиями и без них.\n",
    "\n",
    "df_0 = df[df['is_duplicate'] == 0]\n",
    "df_1 = df[df['is_duplicate'] == 1]\n",
    "\n",
    "name_1_df_0 = list(df_0['name_1'])\n",
    "name_2_df_0 = list(df_0['name_2'])\n",
    "\n",
    "name_1_df_1 = list(df_1['name_1'])\n",
    "name_2_df_1 = list(df_1['name_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69006ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "995638\n"
     ]
    }
   ],
   "source": [
    "# Объединим два столбца, посмотрим сколько всего названий.\n",
    "text = list(df['name_1']) + list(list(df['name_2']))\n",
    "\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d32acc94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18022\n",
      "Количество слов 74073\n",
      "Количество уникальных слов  18177\n"
     ]
    }
   ],
   "source": [
    "# Оставим только уникальные названия.\n",
    "\n",
    "text_unic = list(set(text))\n",
    "\n",
    "print(len(text_unic))\n",
    "\n",
    "# Разобъем все названия на отдельные слова\n",
    "words = []\n",
    "for string in text_unic:\n",
    "    subwords = string.split()\n",
    "    for word in subwords:\n",
    "        words.append(word)\n",
    "\n",
    "print(\"Количество слов\" , len(words))\n",
    "words_unic = list(set(words))\n",
    "print(\"Количество уникальных слов \", len(words_unic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6c7a69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bestocean worldwide logistics inc\n"
     ]
    }
   ],
   "source": [
    "# Посмотрим на результат предобработки\n",
    "print(preprocessing(text[12]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0601399",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                            'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                            \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                            'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                            'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                            'itself', ...])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Будем делать эмбеддинги с помощью TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(analyzer = 'word', stop_words=stop)\n",
    "\n",
    "tfidf_vectorizer.fit(words_unic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ab065ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "carlisle coating & waterproofing inc\n",
      "carlisle coating & wtrprfng\n"
     ]
    }
   ],
   "source": [
    "# Пример текста. Предобработаем его, вычислим эмбеддинги, а потом посмотрим на косинусное сходство.\n",
    "\n",
    "text1 = preprocessing('Carlisle Coatings & Waterproofing, Inc.' )\n",
    "text2 = preprocessing('Carlisle Coatings & Wtrprfng')\n",
    "\n",
    "print(text1)\n",
    "print(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f84a3c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция делает эмбеддинг названия, как сумму эмбеддингов входящих в него слов.\n",
    "def sentence_embedding(string):\n",
    "    sentence_list = [tfidf_vectorizer.transform([word]) for word in string.split()]\n",
    "    result = np.sum(np.array(sentence_list), axis=0)\n",
    "    return result\n",
    "\n",
    "    \n",
    "text1_vec = sentence_embedding(text1)\n",
    "text2_vec = sentence_embedding(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b8c6810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.57735027]]\n",
      "[[1.]]\n"
     ]
    }
   ],
   "source": [
    "# Посмотрим на косинусное сходство двух текстов разных и одинаковых.\n",
    "\n",
    "print(cosine_similarity(text1_vec, text2_vec))\n",
    "print(cosine_similarity(text1_vec, text1_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "68fa8a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вычислим косинусное сходство для всех пар из нашего датасета.\n",
    "\n",
    "pair_0 = [cosine_similarity( sentence_embedding(item[0] ), sentence_embedding(item[1]) ) for item in zip(name_1_df_0 ,name_2_df_0 )]\n",
    "pair_1 = [cosine_similarity( sentence_embedding(item[0]), sentence_embedding(item[1]) ) for item in zip(name_1_df_1 ,name_2_df_1 )]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d919c4f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0.25819889]]), array([[0.57735027]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.5976143]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.28867513]]), array([[0.25]]), array([[0.25819889]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.31622777]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.21821789]]), array([[0.28867513]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.33333333]]), array([[0.]]), array([[0.23570226]]), array([[0.67082039]]), array([[0.42426407]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.26726124]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.3012992]]), array([[0.]]), array([[0.6]]), array([[0.]]), array([[0.]]), array([[0.]])]\n"
     ]
    }
   ],
   "source": [
    "# Посмотрим на косинусное сходство первых 50 непохожиш пар.\n",
    "print(pair_0[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4eb1c165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0.6681531]]), array([[0.35355339]]), array([[0.35355339]]), array([[0.43643578]]), array([[0.25819889]]), array([[0.2236068]]), array([[0.5]]), array([[0.57735027]]), array([[0.25]]), array([[0.57735027]]), array([[0.66666667]]), array([[0.33333333]]), array([[0.40824829]]), array([[0.51639778]]), array([[0.51639778]]), array([[0.71428571]]), array([[1.]]), array([[0.25]]), array([[0.28867513]]), array([[0.81649658]]), array([[0.40824829]]), array([[0.35355339]]), array([[0.6]]), array([[0.33333333]]), array([[0.28867513]]), array([[0.66666667]]), array([[0.]]), array([[0.75]]), array([[1.]]), array([[0.47140452]]), array([[0.35355339]]), array([[0.35355339]]), array([[0.6]]), array([[0.5]]), array([[0.]]), array([[0.35355339]]), array([[0.28867513]]), array([[0.70710678]]), array([[0.70710678]]), array([[0.89442719]]), array([[0.70710678]]), array([[0.15430335]]), array([[0.25]]), array([[0.33333333]]), array([[0.35355339]]), array([[0.8660254]]), array([[0.2236068]]), array([[0.31622777]]), array([[0.5]]), array([[0.5]])]\n"
     ]
    }
   ],
   "source": [
    "# Посмотрим на косинусное сходство первых 50 похожиш пар.\n",
    "print(pair_1[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ae7b15a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "494161\n",
      "3658\n"
     ]
    }
   ],
   "source": [
    "print(len(pair_0))\n",
    "print(len(pair_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fa9255fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество непохожих пар  373625   Доля 75.60794963584743  %\n",
      "Количество похожих пар  2589   Доля 70.77638053581192  %\n"
     ]
    }
   ],
   "source": [
    "# Выведем долю пар схожих и отличных по косинусному сходству по порогу 0.3\n",
    "\n",
    "pair_0_trashold_05 = [i for i in pair_0 if i < 0.3]\n",
    "pair_1_trashold_05 = [i for i in pair_1 if i>= 0.3]\n",
    "\n",
    "print(\"Количество непохожих пар \",len(pair_0_trashold_05), \"  Доля\", len(pair_0_trashold_05)/len(pair_0)*100,\" %\")\n",
    "\n",
    "print(\"Количество похожих пар \", len(pair_1_trashold_05), \"  Доля\", len(pair_1_trashold_05)/len(pair_1)*100, \" %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e03794",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
