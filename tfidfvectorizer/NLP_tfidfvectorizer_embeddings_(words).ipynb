{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e86c0618",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from scipy.linalg import norm\n",
    "from scipy import stats\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity,cosine_distances\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15ae84f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем стоп слова для русского и английского языка\n",
    "stop1 = list(stopwords.words('english'))\n",
    "stop2 = list(stopwords.words('russian'))\n",
    "\n",
    "stop3 = ['africa', 'african', 'afrika', 'antigua', 'arab', 'arabia', 'arabiyyah', 'art', 'aş', 'bahamas', 'bahrayn', 'barbuda', 'belgien', 'belgique', 'belgië', 'bielaruś', 'bissau', 'bosnia', 'bukchosŏn', 'bulgariya', 'burkina', 'cabo', 'cape', 'centrafricaine', 'central', 'channel', 'chosŏn', 'città', 'city', 'comores', 'costa', 'crna', 'côte', 'dawlat', 'del', 'democratic', 'dhivehi', 'dominican', 'dominicana', 'dr', 'druk', 'du', 'démocratique', 'east', 'ecuatorial', 'el', 'emirates', 'equatorial', 'faeroe', 'faso', 'federated', 'french', 'gabonaise', 'gora', 'grenadines', 'guiana', 'helena', 'hellas', 'hercegovína', 'herzegovina', 'holy', 'hong', 'ia', 'imārat', 'islands', 'isle', 'ityop', 'ivoire', 'kingdom', 'kitts', 'kong', 'korea', 'koromi', 'kuwayt', 'kıbrıs', 'lanka', 'lankā', 'leone', 'leste', 'lester', 'lucia', 'macedonia', 'makedonija', 'man', 'marino', 'marshall', 'maɣréb', 'micronesia', 'mongol', 'mueang', 'nam', 'nevis', 'new', 'north', 'papua', 'principe', 'raajje', 'república', 'rica', 'république', 'sahara', 'saint', 'sak', 'salvador', 'san', 'sao', 'saudi', 'see', 'severna', 'sierra', 'solomon', 'soomaaliya', 'south', 'srbija', 'sri', 'state', 'states', 'suid', 'são', 'thai', 'timor', 'tobago', 'tome', 'tomé', 'trinidad', 'uburundi', 'ul', 'uls', 'umān', 'united', 'urdun', 'vatican', 'vaticano', 'velo', 'verde', 'vincent', 'việt', 'western', 'yaman', 'yul', 'zbekiston', 'zealand', 'şūmāl', 'ūdiyyah', 'ελλάς', 'κύπρος', 'беларусь', 'србија']\n",
    "\n",
    "stop = stop1 + stop2 + stop3\n",
    "\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Функция для предобработки. Удаление знаков пуктуации, приведение к нижнему регистру, лемматизация.\n",
    "def preprocessing(line):\n",
    "    line = line.lower()\n",
    "    line = re.sub(r'[.,\"\\'-?:!;]', \"\", line)\n",
    "    line = nltk.word_tokenize(line)\n",
    "    line = ' '.join([lemmatizer.lemmatize(w) for w in line])\n",
    "    return line\n",
    "\n",
    "\n",
    "# Добавим в стоп слова названия стран и слова-формы собственности организаций. \n",
    "df_stop = pd.read_csv('stop_words.csv')\n",
    "df_countries = pd.read_csv('stop_countries.csv', header=None)\n",
    "stop_company = list(df_stop['0'])\n",
    "stop_countries = list(df_countries[1])\n",
    "stop_words = list(set(stop+ stop_company + stop_countries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ee7f220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Читаем наш датасет.\n",
    "df = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfbd8e32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pair_id</th>\n",
       "      <th>name_1</th>\n",
       "      <th>name_2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Iko Industries Ltd.</td>\n",
       "      <td>Enormous Industrial Trade Pvt., Ltd.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Apcotex Industries Ltd.</td>\n",
       "      <td>Technocraft Industries (India) Ltd.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Rishichem Distributors Pvt., Ltd.</td>\n",
       "      <td>Dsa</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Powermax Rubber Factory</td>\n",
       "      <td>Co. One</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Tress A/S</td>\n",
       "      <td>Longyou Industries Park Zhejiang</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pair_id                             name_1  \\\n",
       "0        1                Iko Industries Ltd.   \n",
       "1        2            Apcotex Industries Ltd.   \n",
       "2        3  Rishichem Distributors Pvt., Ltd.   \n",
       "3        4            Powermax Rubber Factory   \n",
       "4        5                          Tress A/S   \n",
       "\n",
       "                                 name_2  is_duplicate  \n",
       "0  Enormous Industrial Trade Pvt., Ltd.             0  \n",
       "1   Technocraft Industries (India) Ltd.             0  \n",
       "2                                   Dsa             0  \n",
       "3                               Co. One             0  \n",
       "4      Longyou Industries Park Zhejiang             0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Посмотрим на данные.\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "894f8a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 497819 entries, 0 to 497818\n",
      "Data columns (total 4 columns):\n",
      " #   Column        Non-Null Count   Dtype \n",
      "---  ------        --------------   ----- \n",
      " 0   pair_id       497819 non-null  int64 \n",
      " 1   name_1        497819 non-null  object\n",
      " 2   name_2        497819 non-null  object\n",
      " 3   is_duplicate  497819 non-null  int64 \n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 15.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c021f5b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    494161\n",
       "1      3658\n",
       "Name: is_duplicate, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Посмотрим сколько одинаковых пар, а сколько разных.\n",
    "df.is_duplicate.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fcaa6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разбиваем датафрейм на на части с копиями и без них.\n",
    "\n",
    "df_0 = df[df['is_duplicate'] == 0]\n",
    "df_1 = df[df['is_duplicate'] == 1]\n",
    "\n",
    "name_1_df_0 = list(df_0['name_1'])\n",
    "name_2_df_0 = list(df_0['name_2'])\n",
    "\n",
    "name_1_df_1 = list(df_1['name_1'])\n",
    "name_2_df_1 = list(df_1['name_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4f2b87d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "995638\n"
     ]
    }
   ],
   "source": [
    "# Объединим два столбца, посмотрим сколько всего названий.\n",
    "text = list(df['name_1']) + list(df['name_2'])\n",
    "\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66069da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество уникальных названий  18022\n",
      "Количество уникальных названий после предобработки 18022\n",
      "Количество слов 73720\n",
      "Количество уникальных слов  16046\n"
     ]
    }
   ],
   "source": [
    "# Оставим только уникальные названия.\n",
    "\n",
    "text_unic = list(set(text))\n",
    "\n",
    "print(\"Количество уникальных названий \" , len(text_unic))\n",
    "\n",
    "\n",
    "text_unic_preprocess = [preprocessing(name) for name in text_unic]\n",
    "\n",
    "print(\"Количество уникальных названий после предобработки\" , len(text_unic_preprocess))\n",
    "\n",
    "# Разобъем все названия на отдельные слова\n",
    "words = []\n",
    "for string in text_unic_preprocess:\n",
    "    subwords = string.split()\n",
    "    for word in subwords:\n",
    "        words.append(word)\n",
    "\n",
    "print(\"Количество слов\" , len(words))\n",
    "words_unic = list(set(words))\n",
    "print(\"Количество уникальных слов \", len(words_unic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cdbd44e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pidilite lanka private ltd\n"
     ]
    }
   ],
   "source": [
    "# Посмотрим на результат предобработки\n",
    "print(text_unic_preprocess[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ad83c99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(stop_words=['тоже', 'vaticano', 'which', 'own', 'vincent',\n",
       "                            'are', \"won't\", 'чуть', 'other', 'есть', 'algeria',\n",
       "                            'srbija', 'armenia', 'cabo', 'с', 'после',\n",
       "                            'zealand', 'haven', 'lankā', 'products', 'further',\n",
       "                            'для', 'burkina', 'тот', 'turkmenistan',\n",
       "                            'démocratique', 'србија', 'morocco',\n",
       "                            'bielaruś, беларусь', 'sri', ...])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Будем делать эмбеддинги с помощью TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(analyzer = 'word', stop_words=stop_words)\n",
    "\n",
    "tfidf_vectorizer.fit(words_unic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2092050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "carlisle coating & waterproofing inc\n",
      "carlisle coating & wtrprfng\n"
     ]
    }
   ],
   "source": [
    "# Пример текста. Предобработаем его, вычислим эмбеддинги, а потом посмотрим на косинусное сходство.\n",
    "\n",
    "text1 = preprocessing('Carlisle Coatings & Waterproofing, Inc.' )\n",
    "text2 = preprocessing('Carlisle Coatings & Wtrprfng')\n",
    "\n",
    "print(text1)\n",
    "print(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2aea8ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция делает эмбеддинг названия, как сумму эмбеддингов входящих в него слов.\n",
    "def sentence_embedding(string):\n",
    "    sentence_list = [tfidf_vectorizer.transform([word]) for word in string.split()]\n",
    "    result = np.sum(np.array(sentence_list), axis=0)\n",
    "    return result\n",
    "\n",
    "    \n",
    "text1_vec = sentence_embedding(text1)\n",
    "text2_vec = sentence_embedding(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d92e5f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.66666667]]\n",
      "[[1.]]\n"
     ]
    }
   ],
   "source": [
    "# Посмотрим на косинусное сходство двух текстов разных и одинаковых.\n",
    "\n",
    "print(cosine_similarity(text1_vec, text2_vec))\n",
    "print(cosine_similarity(text1_vec, text1_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce38dec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вычислим косинусное сходство для всех пар из нашего датасета.\n",
    "\n",
    "pair_0 = [cosine_similarity( sentence_embedding(item[0] ), sentence_embedding(item[1]) ) for item in zip(name_1_df_0 ,name_2_df_0 )]\n",
    "pair_1 = [cosine_similarity( sentence_embedding(item[0]), sentence_embedding(item[1]) ) for item in zip(name_1_df_1 ,name_2_df_1 )]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b54a2d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.35355339]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]])]\n"
     ]
    }
   ],
   "source": [
    "# Посмотрим на косинусное сходство первых 50 непохожиш пар.\n",
    "print(pair_0[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e54f565a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0.89442719]]), array([[0.5]]), array([[1.]]), array([[0.47140452]]), array([[0.28867513]]), array([[0.40824829]]), array([[1.]]), array([[0.5]]), array([[0.70710678]]), array([[0.70710678]]), array([[1.]]), array([[0.70710678]]), array([[0.70710678]]), array([[0.5]]), array([[0.70710678]]), array([[0.89442719]]), array([[1.]]), array([[0.40824829]]), array([[0.5]]), array([[0.70710678]]), array([[1.]]), array([[1.]]), array([[0.5]]), array([[0.5]]), array([[0.70710678]]), array([[0.70710678]]), array([[0.]]), array([[1.]]), array([[1.]]), array([[0.5]]), array([[1.]]), array([[0.57735027]]), array([[0.81649658]]), array([[0.70710678]]), array([[0.]]), array([[0.70710678]]), array([[1.]]), array([[1.]]), array([[0.70710678]]), array([[1.]]), array([[1.]]), array([[0.2236068]]), array([[0.57735027]]), array([[0.5]]), array([[0.70710678]]), array([[1.]]), array([[0.28867513]]), array([[0.70710678]]), array([[1.]]), array([[0.57735027]])]\n"
     ]
    }
   ],
   "source": [
    "# Посмотрим на косинусное сходство первых 50 похожиш пар.\n",
    "print(pair_1[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3af46233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "494161\n",
      "3658\n"
     ]
    }
   ],
   "source": [
    "print(len(pair_0))\n",
    "print(len(pair_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b28928a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество непохожих пар  475005   Доля найденных непохожих пар среди непохожих 96.12353059023275  %\n",
      "Количество похожих пар  3397   Доля найденных похожих пар среди похожих 92.86495352651723  %\n"
     ]
    }
   ],
   "source": [
    "# Выведем долю пар схожих и отличных по косинусному сходству по порогу 0.3\n",
    "\n",
    "pair_0_trashold_05 = [i for i in pair_0 if i < 0.3]\n",
    "pair_1_trashold_05 = [i for i in pair_1 if i>= 0.3]\n",
    "\n",
    "print(\"Количество непохожих пар \",len(pair_0_trashold_05), \"  Доля найденных непохожих пар среди непохожих\", len(pair_0_trashold_05)/len(pair_0)*100,\" %\")\n",
    "\n",
    "print(\"Количество похожих пар \", len(pair_1_trashold_05), \"  Доля найденных похожих пар среди похожих\", len(pair_1_trashold_05)/len(pair_1)*100, \" %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "98d84752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохраним tfidfvectorizer\n",
    "pickle.dump(tfidf_vectorizer, open('vectorizer_3.pickle', \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
