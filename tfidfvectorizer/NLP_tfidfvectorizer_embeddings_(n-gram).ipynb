{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2db6bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from scipy.linalg import norm\n",
    "from scipy import stats\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity,cosine_distances\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "186f11b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем стоп слова для русского и английского языка\n",
    "stop1 = list(stopwords.words('english'))\n",
    "stop2 = list(stopwords.words('russian'))\n",
    "\n",
    "stop3 = ['africa', 'african', 'afrika', 'antigua', 'arab', 'arabia', 'arabiyyah', 'art', 'aş', 'bahamas', 'bahrayn', 'barbuda', 'belgien', 'belgique', 'belgië', 'bielaruś', 'bissau', 'bosnia', 'bukchosŏn', 'bulgariya', 'burkina', 'cabo', 'cape', 'centrafricaine', 'central', 'channel', 'chosŏn', 'città', 'city', 'comores', 'costa', 'crna', 'côte', 'dawlat', 'del', 'democratic', 'dhivehi', 'dominican', 'dominicana', 'dr', 'druk', 'du', 'démocratique', 'east', 'ecuatorial', 'el', 'emirates', 'equatorial', 'faeroe', 'faso', 'federated', 'french', 'gabonaise', 'gora', 'grenadines', 'guiana', 'helena', 'hellas', 'hercegovína', 'herzegovina', 'holy', 'hong', 'ia', 'imārat', 'islands', 'isle', 'ityop', 'ivoire', 'kingdom', 'kitts', 'kong', 'korea', 'koromi', 'kuwayt', 'kıbrıs', 'lanka', 'lankā', 'leone', 'leste', 'lester', 'lucia', 'macedonia', 'makedonija', 'man', 'marino', 'marshall', 'maɣréb', 'micronesia', 'mongol', 'mueang', 'nam', 'nevis', 'new', 'north', 'papua', 'principe', 'raajje', 'república', 'rica', 'république', 'sahara', 'saint', 'sak', 'salvador', 'san', 'sao', 'saudi', 'see', 'severna', 'sierra', 'solomon', 'soomaaliya', 'south', 'srbija', 'sri', 'state', 'states', 'suid', 'são', 'thai', 'timor', 'tobago', 'tome', 'tomé', 'trinidad', 'uburundi', 'ul', 'uls', 'umān', 'united', 'urdun', 'vatican', 'vaticano', 'velo', 'verde', 'vincent', 'việt', 'western', 'yaman', 'yul', 'zbekiston', 'zealand', 'şūmāl', 'ūdiyyah', 'ελλάς', 'κύπρος', 'беларусь', 'србија']\n",
    "\n",
    "stop = stop1 + stop2 + stop3\n",
    "\n",
    "\n",
    "\n",
    "df_stop = pd.read_csv('stop_words.csv')\n",
    "df_countries = pd.read_csv('stop_countries.csv', header=None)\n",
    "stop_company = list(df_stop['0'])\n",
    "stop_countries = list(df_countries[1])\n",
    "stop_words = list(set(stop+ stop_company + stop_countries))\n",
    "\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Функция для предобработки. Удаление знаков пуктуации, приведение к нижнему регистру, лемматизация, исключение стоп слов.\n",
    "def preprocessing(line):\n",
    "    line = line.lower()\n",
    "    line = re.sub(r'[.,\"\\'-?:!;]', \"\", line)\n",
    "    line = nltk.word_tokenize(line)\n",
    "    line = ' '.join([lemmatizer.lemmatize(w) if w not in stop_words else \"\" for w in line])\n",
    "    return line\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "683703eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Читаем наш датасет.\n",
    "df = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbd72295",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pair_id</th>\n",
       "      <th>name_1</th>\n",
       "      <th>name_2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Iko Industries Ltd.</td>\n",
       "      <td>Enormous Industrial Trade Pvt., Ltd.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Apcotex Industries Ltd.</td>\n",
       "      <td>Technocraft Industries (India) Ltd.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Rishichem Distributors Pvt., Ltd.</td>\n",
       "      <td>Dsa</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Powermax Rubber Factory</td>\n",
       "      <td>Co. One</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Tress A/S</td>\n",
       "      <td>Longyou Industries Park Zhejiang</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pair_id                             name_1  \\\n",
       "0        1                Iko Industries Ltd.   \n",
       "1        2            Apcotex Industries Ltd.   \n",
       "2        3  Rishichem Distributors Pvt., Ltd.   \n",
       "3        4            Powermax Rubber Factory   \n",
       "4        5                          Tress A/S   \n",
       "\n",
       "                                 name_2  is_duplicate  \n",
       "0  Enormous Industrial Trade Pvt., Ltd.             0  \n",
       "1   Technocraft Industries (India) Ltd.             0  \n",
       "2                                   Dsa             0  \n",
       "3                               Co. One             0  \n",
       "4      Longyou Industries Park Zhejiang             0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Посмотрим на данные.\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d97cf50d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 497819 entries, 0 to 497818\n",
      "Data columns (total 4 columns):\n",
      " #   Column        Non-Null Count   Dtype \n",
      "---  ------        --------------   ----- \n",
      " 0   pair_id       497819 non-null  int64 \n",
      " 1   name_1        497819 non-null  object\n",
      " 2   name_2        497819 non-null  object\n",
      " 3   is_duplicate  497819 non-null  int64 \n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 15.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c8e0e68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    494161\n",
       "1      3658\n",
       "Name: is_duplicate, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Посмотрим сколько одинаковых пар, а сколько разных.\n",
    "df.is_duplicate.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d0839e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разбиваем датафрейм на на части с копиями и без них.\n",
    "\n",
    "df_0 = df[df['is_duplicate'] == 0]\n",
    "df_1 = df[df['is_duplicate'] == 1]\n",
    "\n",
    "name_1_df_0 = list(df_0['name_1'])\n",
    "name_2_df_0 = list(df_0['name_2'])\n",
    "\n",
    "name_1_df_1 = list(df_1['name_1'])\n",
    "name_2_df_1 = list(df_1['name_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7273f7f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "995638\n"
     ]
    }
   ],
   "source": [
    "# Объединим два столбца, посмотрим сколько всего названий.\n",
    "text = list(df['name_1']) + list(df['name_2'])\n",
    "\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "facb157f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество уникальных названий  18022\n",
      "Количество уникальных названий после предобработки 18022\n"
     ]
    }
   ],
   "source": [
    "# Оставим только уникальные названия.\n",
    "\n",
    "text_unic = list(set(text))\n",
    "\n",
    "print(\"Количество уникальных названий \" , len(text_unic))\n",
    "\n",
    "\n",
    "text_unic_preprocess = [preprocessing(name) for name in text_unic]\n",
    "\n",
    "print(\"Количество уникальных названий после предобработки\" , len(text_unic_preprocess))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbf7d2e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "andhra   \n"
     ]
    }
   ],
   "source": [
    "print(text_unic_preprocess[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e2f1c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#tfidf_vectorizer = TfidfVectorizer(preprocessor=preprocessing, analyzer = 'word',stop_words=stop)\n",
    "tfidf_vectorizer = TfidfVectorizer(analyzer = 'char', ngram_range=(3, 3))\n",
    "\n",
    "\n",
    "tfidf_vectorizer.fit(text_unic_preprocess)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1fe44eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "carlisle coating & waterproofing \n",
      "carlisle coating & wtrprfng\n"
     ]
    }
   ],
   "source": [
    "text1 = preprocessing('Carlisle Coatings & Waterproofing, Inc.' )\n",
    "text2 = preprocessing('Carlisle Coatings & Wtrprfng')\n",
    "\n",
    "print(text1)\n",
    "print(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b555a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.70171479]]\n",
      "[[1.]]\n"
     ]
    }
   ],
   "source": [
    "# Функция делает эмбеддинг названия, как сумму эмбеддингов входящих в него слов.\n",
    "def sentence_embedding(string):\n",
    "    sentence_list = [tfidf_vectorizer.transform([word]) for word in string.split()]\n",
    "    result = np.sum(np.array(sentence_list), axis=0)\n",
    "    return result\n",
    "\n",
    "    \n",
    "text1_vec = sentence_embedding(text1)\n",
    "text2_vec = sentence_embedding(text2)\n",
    "\n",
    "\n",
    "# Посмотрим на косинусное сходство двух текстов разных и одинаковых.\n",
    "print(cosine_similarity(text1_vec, text2_vec))\n",
    "print(cosine_similarity(text1_vec, text1_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0c7f206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вычислим косинусное сходство для всех пар из нашего датасета.\n",
    "\n",
    "pair_0 = [cosine_similarity( sentence_embedding(item[0] ), sentence_embedding(item[1]) ) for item in zip(name_1_df_0 ,name_2_df_0 )]\n",
    "pair_1 = [cosine_similarity( sentence_embedding(item[0]), sentence_embedding(item[1]) ) for item in zip(name_1_df_1 ,name_2_df_1 )] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "717c32f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0.42803682]]), array([[0.59688658]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.40824829]]), array([[0.45647576]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.28867513]]), array([[0.25]]), array([[0.41410918]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.28997237]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.22529643]]), array([[0.23570226]]), array([[0.28867513]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.20800813]]), array([[0.33333333]]), array([[0.]]), array([[0.25819889]]), array([[0.67082039]]), array([[0.25]]), array([[0.]]), array([[0.13335326]]), array([[0.]]), array([[0.28867513]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]]), array([[0.22408811]]), array([[0.35582906]]), array([[0.]]), array([[0.6]]), array([[0.]]), array([[0.]]), array([[0.]])]\n"
     ]
    }
   ],
   "source": [
    "print(pair_0[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5d7734f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0.64362225]]), array([[0.40824829]]), array([[0.35355339]]), array([[0.63245553]]), array([[0.28867513]]), array([[0.30227015]]), array([[0.5]]), array([[0.60952785]]), array([[0.35449197]]), array([[0.70710678]]), array([[0.66666667]]), array([[0.33333333]]), array([[0.40824829]]), array([[0.68661959]]), array([[0.70710678]]), array([[0.77029479]]), array([[1.]]), array([[0.40824829]]), array([[0.28867513]]), array([[0.81649658]]), array([[0.57735027]]), array([[0.35355339]]), array([[0.5]]), array([[0.33333333]]), array([[0.5]]), array([[0.78062909]]), array([[0.]]), array([[0.81649658]]), array([[1.]]), array([[0.46048707]]), array([[0.40824829]]), array([[0.35355339]]), array([[0.67082039]]), array([[0.5]]), array([[0.]]), array([[0.35355339]]), array([[0.40359619]]), array([[1.]]), array([[0.70710678]]), array([[0.89442719]]), array([[0.70710678]]), array([[0.20548669]]), array([[0.25]]), array([[0.41554063]]), array([[0.40824829]]), array([[0.8660254]]), array([[0.28867513]]), array([[0.3083418]]), array([[0.70710678]]), array([[0.5]])]\n"
     ]
    }
   ],
   "source": [
    "print(pair_1[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "169afa3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "494161\n",
      "3658\n"
     ]
    }
   ],
   "source": [
    "print(len(pair_0))\n",
    "print(len(pair_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d50d657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество непохожих пар  331589   Доля найденных непохожих пар среди непохожих 67.10141026912282  %\n",
      "Количество похожих пар  3118   Доля найденных похожих пар среди похожих 85.23783488244942  %\n"
     ]
    }
   ],
   "source": [
    "# Выведем долю пар схожих и отличных по косинусному сходству по порогу 0.3\n",
    "\n",
    "pair_0_trashold_05 = [i for i in pair_0 if i < 0.3]\n",
    "pair_1_trashold_05 = [i for i in pair_1 if i>= 0.3]\n",
    "\n",
    "print(\"Количество непохожих пар \",len(pair_0_trashold_05), \"  Доля найденных непохожих пар среди непохожих\", len(pair_0_trashold_05)/len(pair_0)*100,\" %\")\n",
    "\n",
    "print(\"Количество похожих пар \", len(pair_1_trashold_05), \"  Доля найденных похожих пар среди похожих\", len(pair_1_trashold_05)/len(pair_1)*100, \" %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e36d6d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохраним tfidfvectorizer.\n",
    "pickle.dump(tfidf_vectorizer, open('vectorizer_2.pickle', \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d62bdfa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
